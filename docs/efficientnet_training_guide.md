# EfficientNet Training Guide: From Failure to 93.7% Accuracy

This guide documents three training attempts with EfficientNetB4, including two failures and the research-based solution that achieved excellent results.

---

## Table of Contents
1. [Attempt 1: Preprocessing Failure (1% Accuracy)](#attempt-1-preprocessing-failure)
2. [Attempt 2: Learning Rate Collapse (43% â†’ 14%)](#attempt-2-learning-rate-collapse)
3. [Attempt 3: SUCCESS (93.7% Frozen Baseline)](#attempt-3-success)
4. [Fine-Tuning Research & Best Practices](#fine-tuning-research)
5. [Summary Table](#summary-table)

---

## Attempt 1: Preprocessing Failure (1% Accuracy)

When initially training EfficientNetB4 for transfer learning, the model completely failed to learn, achieving only ~1-5% training and validation accuracy (worse than random guessing for 120 classes = 0.83%).

### Initial Configuration (FAILED)
```yaml
batch_size: 16
learning_rate: 0.001  # Same as ResNet50
optimizer: "adam"
image_size: [380, 380]
trainable_base: false  # Frozen base
```

**Result:**
- Epoch 1: Training accuracy 1.04%, Val accuracy 4.84%
- Model not learning at all

## Root Cause Analysis

### Investigation Steps:

1. **Verified Model Loading** âœ“
   - EfficientNetB4 loads correctly from Keras
   - Pre-trained weights download properly
   - Model architecture intact (475 layers)

2. **Verified Data Loading** âœ“
   - Labels in correct range: 0-119
   - Batch shapes correct
   - No data corruption

3. **Root Cause #1: PREPROCESSING** âŒ
   - **WRONG**: Applied 'torch' mode preprocessing (values: -2.12 to +2.64)
   - **CORRECT**: EfficientNet has preprocessing **built into the model**
   - Must pass **raw pixel values 0-255** (no normalization!)
   - Official `efficientnet.preprocess_input()` is a **placeholder that does nothing**

4. **Root Cause #2: LEARNING RATE** âš ï¸
   - Keras documentation recommends LR=0.01 for frozen base
   - BUT this caused **training collapse** for our specific case (120 classes, 380Ã—380 images)
   - Had to reduce to LR=0.001 (same as ResNet50)

## Solution: Official Keras Recommendations

### Research Findings:

**Source:** [Keras Official: Image classification via fine-tuning with EfficientNet](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/)

**Recommended Learning Rates:**

| Phase | Learning Rate | Notes |
|-------|--------------|-------|
| **Frozen Base Training** | **0.01** (1e-2) | 10x HIGHER than ResNet50! |
| **Fine-Tuning** | **0.00001** (1e-5) | 100x lower than frozen |

### Attempt #1: Official Keras LR=0.01 (FAILED - Training Collapsed!)

```yaml
# EfficientNetB4 Frozen Baseline - ATTEMPT WITH KERAS RECOMMENDATION
batch_size: 32
learning_rate: 0.01  # Keras official recommendation
optimizer: "adam"
image_size: [380, 380]
trainable_base: false
# Preprocessing: Raw 0-255 pixel values âœ“
```

**Result: TRAINING COLLAPSED**
- Epoch 1: 43.10% val_acc âœ“ (peaked!)
- Epoch 2: 24.44% (dropped 18 points!)
- Epoch 3: 15.94%
- Epoch 4: 14.39%
- Training got progressively worse, early stopping at epoch 6

**Conclusion:** Despite Keras documentation recommending LR=0.01, it was **TOO HIGH** for our specific task (120 classes, 380Ã—380 images). The model peaked immediately then collapsed.

### Attempt #2: Reduced LR=0.001 (SUCCESS!)

```yaml
# EfficientNetB4 Frozen Baseline - FINAL WORKING CONFIGURATION
batch_size: 32
learning_rate: 0.001  # Reduced from 0.01 (same as ResNet50)
optimizer: "adam"
image_size: [380, 380]
trainable_base: false
# Preprocessing: Raw 0-255 pixel values âœ“
```

**Result: EXCELLENT PERFORMANCE**
- Epoch 1: 91.31% val_acc
- Epoch 5: 93.00%
- **Epoch 6: 93.12% val_acc** âœ“ (BEST)
- Top-5 accuracy: 99.46%
- Stable training, no collapse

### Key Differences: EfficientNet vs ResNet50

| Aspect | ResNet50 | EfficientNetB4 (WORKING) |
|--------|----------|--------------------------|
| **Preprocessing** | Caffe mode (RGBâ†’BGR) | **Raw 0-255 (built-in)** |
| **Frozen LR** | 0.001 | **0.001** (NOT 0.01!) |
| **Fine-tune LR** | 0.0001 | **0.00001** (10x lower) |
| **Image Size** | 224Ã—224 | **380Ã—380** |
| **Batch Size** | 32 | 32 |
| **Frozen Baseline** | 54.69% | **93.12%** (+38.43%) |
| **Architecture** | Residual blocks | Compound scaling + BN |

## Critical Insight: Keras Documentation vs Reality

**âš ï¸ WARNING:** Keras documentation says LR=0.01 for EfficientNet frozen training, but this **FAILED** in our case!

**Why the discrepancy?**
1. **Different task complexity**: Keras examples use simpler datasets (fewer classes)
2. **Different image sizes**: Standard examples use 224Ã—224, we use 380Ã—380
3. **Dataset-specific**: 120-class fine-grained classification is harder than typical transfer learning
4. **Documentation is generic**: Always validate hyperparameters on YOUR specific task

**Lesson:** Official documentation provides **starting points**, not absolute truths. Always experiment!

## Common Mistakes to Avoid

### âŒ DON'T:
- Apply 'torch' mode preprocessing - EfficientNet has it **built-in**!
- Blindly trust Keras documentation LR=0.01 - it may cause training collapse
- Use same preprocessing as ResNet50 (Caffe mode won't work)
- Expect all EfficientNet variants to behave identically

### âœ“ DO:
- Use **raw 0-255 pixel values** (no normalization)
- Start with LR=0.001 for frozen baseline (safer than 0.01)
- Monitor first few epochs - if accuracy drops, reduce learning rate
- Verify preprocessing is correct before long training runs

## Actual Results

### With Wrong Preprocessing ('torch' mode):
- **FAILURE**: 1-5% accuracy (model doesn't learn)
- Wasted GPU time: ~40 minutes

### With Keras LR=0.01 + Correct Preprocessing:
- **COLLAPSE**: Peaked at 43% then dropped to 14%
- Training deteriorated after epoch 1

### With LR=0.001 + Correct Preprocessing (RAW 0-255):
- **SUCCESS**: 93.12% validation accuracy (frozen baseline!)
- Top-5 accuracy: 99.46%
- Beats ResNet50 fine-tuned (77.68%) by +15.44%

---

## Fine-Tuning Research & Best Practices

After achieving 93.7% frozen baseline, we researched optimal fine-tuning strategies.

### Initial Fine-Tuning Attempt (Suboptimal)

**Configuration:**
```yaml
unfreeze_last_n: 50  # Too many layers
learning_rate: 0.0001  # 1e-4 (too high for fine-tuning)
batch_size: 16
```

**Result:** Only reached **92.9%** validation accuracy (worse than frozen 93.7%!)

**Problems Identified:**
1. âŒ **BatchNormalization layers were being unfrozen** - Destroys learned representations
2. âŒ **Too many layers unfrozen** (50 out of 475) - Causes overfitting
3. âŒ **Learning rate too high** (1e-4 vs recommended 1e-5)
4. âŒ **Not respecting block boundaries** - EfficientNet has skip connections

### Research Findings

**Source:** [Keras Official: Fine-tuning with EfficientNet](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/)

**Key Recommendations:**

1. **BatchNormalization Layers Must Stay Frozen**
   - Unfreezing BatchNorm destroys learned statistics
   - Causes significant accuracy drop in first epoch
   - CRITICAL: Always keep BatchNorm frozen during fine-tuning

2. **Unfreeze Only Top 20 Layers**
   - Not 50 layers (too many)
   - Must respect block boundaries (EfficientNet has skip connections)
   - Keras recommendation: 20 layers for EfficientNet

3. **Learning Rate: 1e-5 (not 1e-4)**
   - Frozen baseline: 1e-2 or 1e-3
   - Fine-tuning: **1e-5** (100x lower than frozen)
   - Our mistake: Used 1e-4 (10x too high)

4. **Smaller Batch Sizes Help**
   - Research shows smaller batches improve validation accuracy
   - Use batch_size=16 (not 32)

5. **Gradual Unfreezing Strategy**
   - Start with top layers
   - Progressively unfreeze earlier layers
   - Minimizes risk of catastrophic forgetting

### Improved Fine-Tuning Configuration (v2)

**[configs/efficientnetb4_finetune_v2.yaml](../configs/efficientnetb4_finetune_v2.yaml)**

```yaml
# Research-based fine-tuning configuration
load_checkpoint: "experiments/exp_20251008_115733/checkpoints/best_model.h5"

# Keras recommendations:
unfreeze_last_n: 20  # Only top 20 layers (not 50!)
learning_rate: 0.00001  # 1e-5 (Keras recommendation, not 1e-4)
batch_size: 16  # Smaller batch size
epochs: 30  # Fewer epochs needed
patience: 8

# BatchNorm freezing implemented in code:
# - All BatchNormalization layers stay frozen
# - Only Conv/Dense layers are trainable
```

**Code Implementation:** [src/dbc/train_cnn.py:148-162](../src/dbc/train_cnn.py#L148-L162)

```python
# Freeze all layers first
base_layer.trainable = True
batchnorm_count = 0
for i, layer in enumerate(base_layer.layers):
    if i >= freeze_until:
        # Unfreeze this layer, but keep BatchNormalization frozen
        if 'BatchNormalization' in layer.__class__.__name__:
            layer.trainable = False  # CRITICAL!
            batchnorm_count += 1
        else:
            layer.trainable = True
    else:
        layer.trainable = False

if batchnorm_count > 0:
    print(f"  Kept {batchnorm_count} BatchNormalization layers frozen")
```

### Why BatchNorm Freezing is Critical

**What happens if you unfreeze BatchNorm:**
- BatchNorm layers have running mean/variance statistics
- These were computed on ImageNet (1000 classes)
- Fine-tuning updates these statistics for your task (120 dog breeds)
- BUT: Small batch size (16) gives poor statistics estimates
- Result: Model forgets ImageNet features, performance drops

**Best Practice:**
- Keep BatchNorm in **inference mode** (trainable=False)
- Only update Conv/Dense layer weights
- Preserves learned feature distributions

## References

1. **Keras Official Guide**: [Image classification via fine-tuning with EfficientNet](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/)
2. **StackOverflow**: [EfficientNetV2 convergence issues](https://stackoverflow.com/questions/75048164/keras-efficientnetv2-doesnt-converge-while-efficientnet-does) - Confirms 1e-4 fixes convergence
3. **Research**: EfficientNet paper shows compound scaling requires adjusted hyperparameters

## Lessons Learned

1. **EfficientNet preprocessing is BUILT-IN** - Must pass raw 0-255 pixel values, not normalized
2. **Official documentation isn't always right for your task** - Keras LR=0.01 caused collapse, LR=0.001 worked
3. **Verify preprocessing first** - Wrong preprocessing (torch mode) gave 1% accuracy vs 93% with correct
4. **Monitor early epochs** - Training collapse shows up in first few epochs (peaked then dropped)
5. **Start conservative, then experiment** - LR=0.001 safer than LR=0.01 despite documentation
6. **Context matters** - 120 classes + 380Ã—380 images behaves differently than typical examples

---

## Summary Table

### Training Attempts

| Attempt | Phase | Preprocessing | Learning Rate | Layers Unfrozen | BatchNorm Frozen? | Result |
|---------|-------|---------------|---------------|-----------------|-------------------|--------|
| **1** | Frozen | 'torch' mode âŒ | 0.001 | 0 (frozen) | N/A | âŒ **1-5% accuracy** |
| **2** | Frozen | Raw 0-255 âœ“ | 0.01 âŒ | 0 (frozen) | N/A | âŒ **43% â†’ 14% collapse** |
| **3** | Frozen | Raw 0-255 âœ“ | 0.001 âœ“ | 0 (frozen) | N/A | âœ… **93.7% SUCCESS!** |
| **4** | Fine-tune | Raw 0-255 âœ“ | 0.0001 âŒ | 50 âŒ | No âŒ | âš ï¸ **92.9% (worse than frozen)** |
| **5** | Fine-tune | Raw 0-255 âœ“ | 0.00001 âœ“ | 20 âœ“ | Yes âœ“ | ðŸ”„ **To be tested** |

### Key Learnings by Hyperparameter

| Hyperparameter | Wrong Value | Correct Value | Impact |
|----------------|-------------|---------------|--------|
| **Preprocessing** | 'torch' mode normalization | Raw 0-255 (built-in) | 1% â†’ 93% accuracy |
| **LR (frozen)** | 0.01 (Keras docs) | 0.001 | Prevents collapse (43%â†’14%) |
| **LR (fine-tune)** | 0.0001 (1e-4) | 0.00001 (1e-5) | Better convergence |
| **Layers unfrozen** | 50 layers | 20 layers | Prevents overfitting |
| **BatchNorm** | Trainable | Frozen | Preserves learned stats |

---

## Final Results

**Frozen Baseline (Best):**
- **Validation Accuracy: 93.71%**
- **Top-5 Accuracy: 99.63%**
- Training time: ~49 minutes (20 epochs)
- Config: [efficientnetb4_baseline.yaml](../configs/efficientnetb4_baseline.yaml)

**Comparison with ResNet50:**
- EfficientNetB4 frozen: **93.71%**
- ResNet50 fine-tuned: 77.68%
- **Improvement: +16.03%**

**Fine-Tuning Status:**
- Attempt 4 (LR=1e-4, 50 layers, no BatchNorm freezing): 92.9% âŒ
- Attempt 5 (LR=1e-5, 20 layers, BatchNorm frozen): Pending ðŸ”„

---

## Recommendations for Future Work

1. **For Similar Tasks (120+ classes, large images):**
   - Start with frozen baseline first
   - Use conservative learning rate (1e-3 for frozen, 1e-5 for fine-tuning)
   - EfficientNetB4 frozen may be sufficient (93.7% is excellent!)

2. **If Fine-Tuning EfficientNet:**
   - âœ… ALWAYS keep BatchNorm layers frozen
   - âœ… Use 1e-5 learning rate (not 1e-4)
   - âœ… Unfreeze only 20 layers (not 50)
   - âœ… Use smaller batch sizes (16 works well)
   - âœ… Monitor first 5 epochs closely for collapse

3. **Red Flags:**
   - Training accuracy >> Validation accuracy â†’ Overfitting, reduce layers/add regularization
   - Accuracy drops after first epoch â†’ Learning rate too high or BatchNorm unfrozen
   - Validation accuracy worse than frozen baseline â†’ Fine-tuning hurting, not helping

4. **When to Stop:**
   - If frozen baseline achieves target accuracy (like 93.7%), consider skipping fine-tuning
   - Fine-tuning is not always better, especially with excellent frozen results

---

**Last Updated**: 2025-10-08
**Project**: Dog Breed Classification (Stanford Dogs Dataset)
**Best Result**: EfficientNetB4 Frozen Baseline - 93.71% validation accuracy
