# EfficientNetB0 Fine-Tuning - Unfrozen Training
# Expected performance: 80-87% validation accuracy (best expected with fine-tuning)

# Experiment metadata
experiment_name: "EfficientNetB0 Fine-Tuning"
experiment_description: "Fine-tuning with unfrozen EfficientNetB0 base, 20 epochs, lower learning rate"
experiment_tags: ["finetune", "efficientnet", "efficientnetb0", "transfer", "unfrozen"]

paths:
  data: "data/raw"
  artifacts: "artifacts"
  experiments: "experiments"

train:
  # Model architecture
  model_type: "transfer"
  base_model: "efficientnetb0"
  trainable_base: false   # Will use selective unfreezing via finetune_from_checkpoint.py
  unfreeze_last_n_layers: 20  # Unfreeze last 20 layers (conservative)

  # Training parameters
  batch_size: 64  # Reduced batch size for fine-tuning stability
  epochs: 40
  learning_rate: 0.00001  # Very low LR for fine-tuning (100x lower than frozen)
  optimizer: "adam"

  # Regularization
  dropout: 0.5

  # Early stopping
  early_stopping: true
  patience: 7

  # Data
  image_size: [224, 224]
  num_classes: 120
