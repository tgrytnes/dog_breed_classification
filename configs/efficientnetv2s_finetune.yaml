# EfficientNetV2-S Fine-Tuning
# Phase 2: Fine-tune with selective layer unfreezing

experiment_name: "EfficientNetV2-S Fine-Tuning - Selective (20 layers)"
experiment_description: "Fine-tuning EfficientNetV2-S with last 20 layers unfrozen, lower learning rate"
experiment_tags: ["finetune", "efficientnetv2s", "transfer", "selective"]

paths:
  data: "data/raw"
  artifacts: "artifacts"
  experiments: "experiments"

train:
  # Model architecture
  model_type: "transfer"
  base_model: "efficientnetv2s"
  load_checkpoint: "experiments/exp_20251008_174650/checkpoints/best_model.h5"  # Update with your baseline checkpoint
  unfreeze_last_n: 20  # Unfreeze top 20 layers

  # Training parameters
  batch_size: 16  # Keep same as baseline due to 384x384 images
  epochs: 10  # As per your requirement
  learning_rate: 0.00001  # 1e-5 (100x lower than baseline 1e-3)
  optimizer: "adam"

  # Regularization
  dropout: 0.5

  # Learning rate schedule
  lr_schedule: "cosine_warmup"
  warmup_epochs: 2
  min_learning_rate: 0.000001  # 1e-6

  # Early stopping
  early_stopping: true
  patience: 5

  # Data
  image_size: [384, 384]  # EfficientNetV2-S uses 384x384
  num_classes: 120
